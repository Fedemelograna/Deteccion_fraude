# üí≥ Proyecto: Optimizaci√≥n de la Detecci√≥n de Fraude con XGBoost

## üí° Resumen del Proyecto
Este proyecto aborda un problema de **clasificaci√≥n binaria** de alta criticidad: la detecci√≥n de transacciones fraudulentas con tarjeta de cr√©dito en un entorno de **extremo desbalance de clases**.

El objetivo principal fue superar las limitaciones de un modelo lineal (Regresi√≥n Log√≠stica) para lograr una **alta Precisi√≥n (Fiabilidad)**, minimizando las falsas alarmas, sin comprometer una elevada tasa de detecci√≥n (Recall).

## üéØ Objetivos de Negocio

La fiabilidad de las alertas es clave para la eficiencia del equipo de riesgo. El modelo deb√≠a cumplir con los siguientes requisitos:

| M√©trica | Requisito M√≠nimo | Impacto en el Negocio |
| :--- | :--- | :--- |
| **Precisi√≥n (P)** | $\mathbf{\geq 0.70}$ | **Fiabilidad:** Reducir las Falsas Alarmas (transacciones leg√≠timas marcadas como fraude). |
| **Recall (R)** | $\mathbf{\geq 0.85}$ | **Detecci√≥n:** Capturar la mayor cantidad posible de casos de fraude real. |

## üìä Descripci√≥n del Dataset y Desaf√≠o

El proyecto se realiz√≥ sobre un dataset de transacciones de tarjeta de cr√©dito (obtenido de Kaggle).

* **Total de Transacciones:** 284,807.
* **Fraudes Registrados:** 492.
* **Desaf√≠o Clave (Desbalance):** El dataset presenta un severo desbalance de $\mathbf{1:577}$ (1 caso de fraude por cada 577 transacciones leg√≠timas).
* **Variables:** √önicamente variables num√©ricas (V1 a V28) resultantes de una transformaci√≥n PCA.

## üõ†Ô∏è Metodolog√≠a de Modelado

Se implement√≥ una metodolog√≠a de dos fases para demostrar el valor de un modelo no lineal.

### 1. Modelo Benchmark: Regresi√≥n Log√≠stica (RL)

* **Resultado:** El mejor *trade-off* nos ofreci√≥ solo un $\mathbf{P=0.55}$ para un $\mathbf{R=0.88}$.
* **Conclusi√≥n:** El modelo lineal genera un $\mathbf{45\%}$ de falsas alarmas, fallando en el objetivo de Precisi√≥n.
* **Insight:** El an√°lisis de coeficientes de la RL identific√≥ a **V14, V10, y V17** como las variables m√°s cr√≠ticas, lo que justific√≥ el uso de un modelo que capture interacciones complejas.

### 2. Modelo Avanzado: XGBoost Classifier

Para romper la barrera del $55\%$ de Precisi√≥n, se utiliz√≥ un **XGBoost Classifier** optimizado para datos desbalanceados, utilizando el par√°metro `scale_pos_weight = 577`.

## üìà Resultados Finales

El modelo XGBoost super√≥ los objetivos de negocio. La decisi√≥n final del umbral de clasificaci√≥n se bas√≥ en el an√°lisis de la curva **Precision-Recall** para obtener el mejor F1-Score general.

### M√©tricas de Rendimiento Final (Umbral Operativo: 0.5)

| M√©trica | Regresi√≥n Log√≠stica | **XGBoost Final** | Meta Cumplida |
| :--- | :--- | :--- | :--- |
| **Precisi√≥n (P)** | $0.55$ | $\mathbf{0.76}$ | ‚úÖ (Meta: $\geq 0.70$) |
| **Recall (R)** | $0.88$ | $\mathbf{0.84}$ | üü° (Cerca de Meta: $\geq 0.85$) |
| **F1-Score** | $0.68$ | $\mathbf{0.80}$ | *(El mejor balance de ambos modelos)* |


## üöÄ Impacto Operacional

La mejora en la Precisi√≥n se traduce directamente en una mayor eficiencia para el equipo de riesgo:

* **Reducci√≥n de Falsas Alarmas:** La tasa de falsas alarmas se redujo de un $45\%$ (RL) a solo un $\mathbf{24\%}$ (XGBoost).
* **Ahorro de Tiempo:** Esto representa una reducci√≥n de $\mathbf{47\%}$ en el volumen de transacciones de no-fraude que requieren revisi√≥n manual, permitiendo al equipo enfocarse en casos de alto riesgo.

## ‚öôÔ∏è C√≥mo Ejecutar el Proyecto

Este proyecto fue desarrollado en un entorno de Google Colab (`.ipynb`).

### Prerequisitos

* Python 3.x
* Librer√≠as principales:
    ```bash
    pip install pandas numpy scikit-learn xgboost matplotlib
    ```

### Pasos para Replicar

1.  Clonar este repositorio:
    ```bash
    git clone [https://docs.github.com/es/repositories/creating-and-managing-repositories/quickstart-for-repositories](https://docs.github.com/es/repositories/creating-and-managing-repositories/quickstart-for-repositories)
    ```
2.  Abrir el archivo `Proyecto_detecci√≥n_de_fraude.ipynb` en Google Colab.
3.  El notebook contiene los pasos de limpieza, entrenamiento de ambos modelos (RL y XGBoost), y la optimizaci√≥n de umbrales.
